{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:53:03.839326Z","iopub.execute_input":"2025-04-27T17:53:03.839646Z","iopub.status.idle":"2025-04-27T17:53:06.056175Z","shell.execute_reply.started":"2025-04-27T17:53:03.839612Z","shell.execute_reply":"2025-04-27T17:53:06.055237Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ieee-fraud-detection/sample_submission.csv\n/kaggle/input/ieee-fraud-detection/test_identity.csv\n/kaggle/input/ieee-fraud-detection/train_identity.csv\n/kaggle/input/ieee-fraud-detection/test_transaction.csv\n/kaggle/input/ieee-fraud-detection/train_transaction.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install dagshub mlflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:53:06.058802Z","iopub.execute_input":"2025-04-27T17:53:06.059211Z","iopub.status.idle":"2025-04-27T17:53:22.621884Z","shell.execute_reply.started":"2025-04-27T17:53:06.059186Z","shell.execute_reply":"2025-04-27T17:53:22.620632Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting dagshub\n  Downloading dagshub-0.5.9-py3-none-any.whl.metadata (12 kB)\nCollecting mlflow\n  Downloading mlflow-2.22.0-py3-none-any.whl.metadata (30 kB)\nRequirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\nCollecting appdirs>=1.4.4 (from dagshub)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.1.8)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\nRequirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.44)\nRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (14.0.0)\nCollecting dacite~=1.6.0 (from dagshub)\n  Downloading dacite-1.6.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (9.0.0)\nCollecting gql[requests] (from dagshub)\n  Downloading gql-3.5.2-py2.py3-none-any.whl.metadata (9.4 kB)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.3)\nCollecting treelib>=1.6.4 (from dagshub)\n  Downloading treelib-1.7.1-py3-none-any.whl.metadata (1.4 kB)\nCollecting pathvalidate>=3.0.0 (from dagshub)\n  Downloading pathvalidate-3.2.3-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.37.29)\nRequirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\nCollecting dagshub-annotation-converter>=0.1.5 (from dagshub)\n  Downloading dagshub_annotation_converter-0.1.9-py3-none-any.whl.metadata (2.5 kB)\nCollecting mlflow-skinny==2.22.0 (from mlflow)\n  Downloading mlflow_skinny-2.22.0-py3-none-any.whl.metadata (31 kB)\nRequirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\nCollecting graphene<4 (from mlflow)\n  Downloading graphene-3.4.3-py2.py3-none-any.whl.metadata (6.9 kB)\nCollecting gunicorn<24 (from mlflow)\n  Downloading gunicorn-23.0.0-py3-none-any.whl.metadata (4.4 kB)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7.5)\nRequirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (5.5.2)\nRequirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.1)\nCollecting databricks-sdk<1,>=0.20.0 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading databricks_sdk-0.50.0-py3-none-any.whl.metadata (38 kB)\nCollecting fastapi<1 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading fastapi-0.115.12-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.6.1)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.16.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.16.0)\nRequirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (24.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.20.3)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.11.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (4.13.1)\nCollecting uvicorn<1 (from mlflow-skinny==2.22.0->mlflow)\n  Downloading uvicorn-0.34.2-py3-none-any.whl.metadata (6.5 kB)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.3.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.1.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.6-py3-none-any.whl.metadata (11 kB)\nCollecting graphql-relay<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_relay-3.2.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\nRequirement already satisfied: botocore<1.38.0,>=1.37.29 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.37.29)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.11.4)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\nCollecting graphql-core<3.3,>=3.1 (from graphene<4->mlflow)\n  Downloading graphql_core-3.2.4-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.19.0)\nCollecting backoff<3.0,>=1.11.1 (from gql[requests]->dagshub)\n  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\nRequirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (2.27.0)\nCollecting starlette<0.47.0,>=0.40.0 (from fastapi<1->mlflow-skinny==2.22.0->mlflow)\n  Downloading starlette-0.46.2-py3-none-any.whl.metadata (6.2 kB)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.2.18)\nRequirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (75.1.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (0.37b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.4.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.0.0)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.2.0)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.17.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (4.9)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.6.1)\nDownloading dagshub-0.5.9-py3-none-any.whl (260 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m260.1/260.1 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading mlflow-2.22.0-py3-none-any.whl (29.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m29.0/29.0 MB\u001b[0m \u001b[31m51.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading mlflow_skinny-2.22.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m77.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading dacite-1.6.0-py3-none-any.whl (12 kB)\nDownloading dagshub_annotation_converter-0.1.9-py3-none-any.whl (33 kB)\nDownloading graphene-3.4.3-py2.py3-none-any.whl (114 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.9/114.9 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gunicorn-23.0.0-py3-none-any.whl (85 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.0/85.0 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading pathvalidate-3.2.3-py3-none-any.whl (24 kB)\nDownloading treelib-1.7.1-py3-none-any.whl (19 kB)\nDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\nDownloading databricks_sdk-0.50.0-py3-none-any.whl (692 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m692.3/692.3 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fastapi-0.115.12-py3-none-any.whl (95 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_core-3.2.4-py3-none-any.whl (203 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m203.2/203.2 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading graphql_relay-3.2.0-py3-none-any.whl (16 kB)\nDownloading uvicorn-0.34.2-py3-none-any.whl (62 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading gql-3.5.2-py2.py3-none-any.whl (74 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading starlette-0.46.2-py3-none-any.whl (72 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: appdirs, uvicorn, treelib, pathvalidate, gunicorn, graphql-core, dacite, backoff, starlette, graphql-relay, gql, graphene, fastapi, databricks-sdk, mlflow-skinny, dagshub-annotation-converter, mlflow, dagshub\n  Attempting uninstall: dacite\n    Found existing installation: dacite 1.9.2\n    Uninstalling dacite-1.9.2:\n      Successfully uninstalled dacite-1.9.2\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nydata-profiling 4.16.1 requires dacite>=1.8, but you have dacite 1.6.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 backoff-2.2.1 dacite-1.6.0 dagshub-0.5.9 dagshub-annotation-converter-0.1.9 databricks-sdk-0.50.0 fastapi-0.115.12 gql-3.5.2 graphene-3.4.3 graphql-core-3.2.4 graphql-relay-3.2.0 gunicorn-23.0.0 mlflow-2.22.0 mlflow-skinny-2.22.0 pathvalidate-3.2.3 starlette-0.46.2 treelib-1.7.1 uvicorn-0.34.2\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='zeliz22', repo_name='ML_Fraud-Detection', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:53:22.623241Z","iopub.execute_input":"2025-04-27T17:53:22.623630Z","iopub.status.idle":"2025-04-27T17:53:28.710176Z","shell.execute_reply.started":"2025-04-27T17:53:22.623594Z","shell.execute_reply":"2025-04-27T17:53:28.708961Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"                                       \u001b[1m❗❗❗ AUTHORIZATION REQUIRED ❗❗❗\u001b[0m                                        \n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">                                       <span style=\"font-weight: bold\">❗❗❗ AUTHORIZATION REQUIRED ❗❗❗</span>                                        \n</pre>\n"},"metadata":{}},{"name":"stdout","text":"\n\nOpen the following link in your browser to authorize the client:\nhttps://dagshub.com/login/oauth/authorize?state=57349f39-5612-471e-b287-a02c51749bb9&client_id=32b60ba385aa7cecf24046d8195a71c07dd345d9657977863b52e7748e0f0f28&middleman_request_id=175d294b60223e7b27051057c5ee95ec5072f3500cc877b2eb65331e5eafebfd\n\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Output()","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Accessing as zeliz22\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Accessing as zeliz22\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"zeliz22/ML_Fraud-Detection\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"zeliz22/ML_Fraud-Detection\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository zeliz22/ML_Fraud-Detection initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository zeliz22/ML_Fraud-Detection initialized!\n</pre>\n"},"metadata":{}}],"execution_count":3},{"cell_type":"code","source":"train_transaction =  pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntrain_identity =  pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:53:28.711385Z","iopub.execute_input":"2025-04-27T17:53:28.712135Z","iopub.status.idle":"2025-04-27T17:54:01.398499Z","shell.execute_reply.started":"2025-04-27T17:53:28.712096Z","shell.execute_reply":"2025-04-27T17:54:01.397588Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"train_merged = pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n\nprint(train_merged.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:01.401215Z","iopub.execute_input":"2025-04-27T17:54:01.401548Z","iopub.status.idle":"2025-04-27T17:54:02.383351Z","shell.execute_reply.started":"2025-04-27T17:54:01.401505Z","shell.execute_reply":"2025-04-27T17:54:02.381419Z"}},"outputs":[{"name":"stdout","text":"(590540, 434)\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef prepare_data(df, target='isFraud', test_size=0.15, random_state=42):\n    # Split train/val/test\n    train_val, test = train_test_split(\n        df, test_size=test_size, stratify=df[target], random_state=random_state\n    )\n    train, val = train_test_split(\n        train_val, \n        test_size=test_size/(1-test_size),  # Adjust for nested split\n        stratify=train_val[target],\n        random_state=random_state\n    )\n    \n    # Separate X/y\n    def _split(df):\n        return df.drop(columns=[target, 'TransactionID']), df[target]\n    \n    X_train, y_train = _split(train)\n    X_val, y_val = _split(val)\n    X_test, y_test = _split(test)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test\n\nX_train, X_val, X_test, y_train, y_val, y_test = prepare_data(train_merged)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:02.384658Z","iopub.execute_input":"2025-04-27T17:54:02.385024Z","iopub.status.idle":"2025-04-27T17:54:09.862636Z","shell.execute_reply.started":"2025-04-27T17:54:02.384991Z","shell.execute_reply":"2025-04-27T17:54:09.861782Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Optional, Union\n\nclass DataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, \n                 numeric_strategy: str = 'mean',\n                 categorical_strategy: str = 'most_frequent',\n                 numeric_fill_value: Optional[Union[int, float]] = None,\n                 categorical_fill_value: Optional[str] = None,\n                 drop_threshold: float = 0.8):\n\n        self.numeric_strategy = numeric_strategy\n        self.categorical_strategy = categorical_strategy\n        self.numeric_fill_value = numeric_fill_value\n        self.categorical_fill_value = categorical_fill_value\n        self.drop_threshold = drop_threshold\n        self.numeric_impute_values_ = {}\n        self.categorical_impute_values_ = {}\n        self.columns_to_drop_ = []\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"Learn imputation values from the data\"\"\"\n        \n        # Identify columns to drop\n        null_ratios = X.isnull().mean()\n        self.columns_to_drop_ = list(null_ratios[null_ratios > self.drop_threshold].index)\n        X_clean = X.drop(columns=self.columns_to_drop_)\n        \n        # Separate numeric and categorical columns\n        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n        \n        # Calculate numeric imputation values\n        for col in numeric_cols:\n            if self.numeric_strategy == 'mean':\n                self.numeric_impute_values_[col] = X_clean[col].mean()\n            elif self.numeric_strategy == 'median':\n                self.numeric_impute_values_[col] = X_clean[col].median()\n            elif self.numeric_strategy == 'constant':\n                if self.numeric_fill_value is None:\n                    raise ValueError(\"numeric_fill_value must be specified for constant strategy\")\n                self.numeric_impute_values_[col] = self.numeric_fill_value\n            elif self.numeric_strategy != 'drop':\n                raise ValueError(f\"Unknown numeric strategy: {self.numeric_strategy}\")\n        \n        # Calculate categorical imputation values\n        for col in categorical_cols:\n            if self.categorical_strategy == 'most_frequent':\n                self.categorical_impute_values_[col] = X_clean[col].mode()[0]\n            elif self.categorical_strategy == 'constant':\n                if self.categorical_fill_value is None:\n                    raise ValueError(\"categorical_fill_value must be specified for constant strategy\")\n                self.categorical_impute_values_[col] = self.categorical_fill_value\n            elif self.categorical_strategy != 'drop':\n                raise ValueError(f\"Unknown categorical strategy: {self.categorical_strategy}\")\n        \n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply the learned imputation to new data\"\"\"\n        \n        # Drop high-null columns\n        X_clean = X.drop(columns=self.columns_to_drop_)\n        \n        # Separate numeric and categorical columns\n        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n        \n        # Apply numeric imputation\n        for col in numeric_cols:\n            if col in self.numeric_impute_values_:\n                X_clean[col] = X_clean[col].fillna(self.numeric_impute_values_[col])\n            elif self.numeric_strategy == 'drop':\n                X_clean = X_clean.dropna(subset=[col])\n        \n        # Apply categorical imputation\n        for col in categorical_cols:\n            if col in self.categorical_impute_values_:\n                X_clean[col] = X_clean[col].fillna(self.categorical_impute_values_[col])\n            elif self.categorical_strategy == 'drop':\n                X_clean = X_clean.dropna(subset=[col])\n        \n        return X_clean\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n        \"\"\"Fit and transform in one step\"\"\"\n        return self.fit(X, y).transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:09.865654Z","iopub.execute_input":"2025-04-27T17:54:09.866088Z","iopub.status.idle":"2025-04-27T17:54:09.881418Z","shell.execute_reply.started":"2025-04-27T17:54:09.866065Z","shell.execute_reply":"2025-04-27T17:54:09.880605Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import mlflow\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass AdvancedDataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, missing_threshold=1):\n        self.missing_threshold = missing_threshold\n        self.numeric_fill = -999\n        self.categorical_fill = \"MISSING\"\n        self.columns_dropped = []\n        self.missing_stats = {}\n        \n    def fit(self, X, y=None):\n        # Calculate missing percentages\n        missing_percent = X.isnull().mean()\n        self.missing_stats = missing_percent.to_dict()\n        \n        # Identify columns to drop\n        self.columns_dropped = list(missing_percent[missing_percent > self.missing_threshold].index)\n        self.columns_kept = [col for col in X.columns if col not in self.columns_dropped]\n        \n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        \n        # 1. Drop high-missing columns\n        X = X.drop(columns=self.columns_dropped)\n        \n        # 2. Fill remaining missing values\n        num_cols = X.select_dtypes(include=['number']).columns\n        cat_cols = X.select_dtypes(exclude=['number']).columns\n        \n        X[num_cols] = X[num_cols].fillna(self.numeric_fill)\n        X[cat_cols] = X[cat_cols].fillna(self.categorical_fill)\n        \n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:09.882436Z","iopub.execute_input":"2025-04-27T17:54:09.882716Z","iopub.status.idle":"2025-04-27T17:54:13.476919Z","shell.execute_reply.started":"2025-04-27T17:54:09.882684Z","shell.execute_reply":"2025-04-27T17:54:13.475858Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass MissingValueHandler(BaseEstimator, TransformerMixin):\n    def __init__(self, num_strategy='median', cat_strategy='most_frequent', \n                 create_flags=True, flag_threshold=0.01, flag_only=False):\n        \"\"\"\n        Optimized missing value handler that avoids fragmentation warnings\n        \n        Parameters same as before\n        \"\"\"\n        self.num_strategy = num_strategy\n        self.cat_strategy = cat_strategy\n        self.create_flags = create_flags\n        self.flag_threshold = flag_threshold\n        self.flag_only = flag_only\n        \n    def fit(self, X, y=None):\n        # Safely detect column types\n        self.num_cols_ = X.select_dtypes(include=np.number).columns.tolist()\n        self.cat_cols_ = X.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        # Initialize storage\n        self.impute_values_ = {}\n        self.flag_cols_ = []\n        \n        # Process all columns with missing values\n        for col in X.columns:\n            missing_ratio = X[col].isna().mean()\n            \n            if missing_ratio > 1e-6:  # Small epsilon to avoid float precision issues\n                # Flag creation logic\n                if self.create_flags and missing_ratio >= self.flag_threshold:\n                    self.flag_cols_.append(col)\n                \n                # Imputation value calculation (unless flag_only)\n                if not self.flag_only:\n                    if col in self.num_cols_:\n                        if self.num_strategy == 'median':\n                            self.impute_values_[col] = X[col].median()\n                        elif self.num_strategy == 'mean':\n                            self.impute_values_[col] = X[col].mean()\n                        else:  # constant\n                            self.impute_values_[col] = 0\n                    elif col in self.cat_cols_:\n                        if self.cat_strategy == 'most_frequent':\n                            # Handle case where mode might be empty\n                            mode = X[col].mode()\n                            self.impute_values_[col] = mode[0] if not mode.empty else 'missing'\n                        else:\n                            self.impute_values_[col] = 'missing'\n        \n        return self\n    \n    def transform(self, X):\n        # Create a single copy upfront\n        X = X.copy()\n        \n        if self.create_flags and len(self.flag_cols_) > 0:\n            # Create all flags at once using pd.concat (more efficient)\n            flag_data = {f'{col}_missing_flag': X[col].isna().astype(np.int8) \n                         for col in self.flag_cols_}\n            X = pd.concat([X, pd.DataFrame(flag_data)], axis=1)\n        \n        # Perform imputation (unless flag_only)\n        if not self.flag_only:\n            for col, value in self.impute_values_.items():\n                if col in X.columns:  # Safety check\n                    X[col] = X[col].fillna(value)\n        \n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:13.477965Z","iopub.execute_input":"2025-04-27T17:54:13.478594Z","iopub.status.idle":"2025-04-27T17:54:13.490594Z","shell.execute_reply.started":"2025-04-27T17:54:13.478561Z","shell.execute_reply":"2025-04-27T17:54:13.489853Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nclass CustomEncoder:\n    def __init__(self, threshold = 3):\n        self.threshold = threshold\n        \n        # Initialize encoders\n        self.one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n        self.ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n        \n        # Store feature names for one-hot encoding\n        self.one_hot_feature_names = None\n        \n    def fit(self, X, y=None):\n\n        cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n        s = X[cat_cols].nunique()\n\n        self.ordinal_cols = list(s[s > self.threshold].index)\n        self.one_hot_cols = list(s[s <= self.threshold].index)\n\n        if self.one_hot_cols:\n            self.one_hot_encoder.fit(X[self.one_hot_cols])\n            self.one_hot_feature_names = self.one_hot_encoder.get_feature_names_out(self.one_hot_cols)\n        \n        if self.ordinal_cols:\n            self.ordinal_encoder.fit(X[self.ordinal_cols])\n        \n        return self\n    \n    def transform(self, X):\n\n        X_transformed = X.copy()\n        \n        # Apply One-Hot Encoding\n        if self.one_hot_cols:\n            one_hot_encoded = self.one_hot_encoder.transform(X[self.one_hot_cols])\n            one_hot_df = pd.DataFrame(one_hot_encoded, columns=self.one_hot_feature_names, index=X.index)\n            X_transformed = pd.concat([X_transformed, one_hot_df], axis=1)\n            X_transformed.drop(self.one_hot_cols, axis=1, inplace=True)\n        \n        # Apply Ordinal Encoding\n        if self.ordinal_cols:\n            ordinal_encoded = self.ordinal_encoder.transform(X[self.ordinal_cols])\n            ordinal_df = pd.DataFrame(ordinal_encoded, columns=self.ordinal_cols, index=X.index)\n            X_transformed[self.ordinal_cols] = ordinal_df\n        \n        return X_transformed\n    \n    def fit_transform(self, X, y = None):\n        return self.fit(X).transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:13.491609Z","iopub.execute_input":"2025-04-27T17:54:13.492175Z","iopub.status.idle":"2025-04-27T17:54:13.530054Z","shell.execute_reply.started":"2025-04-27T17:54:13.492151Z","shell.execute_reply":"2025-04-27T17:54:13.529100Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CorrelationFeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.8):\n        self.threshold = threshold\n        self.features_to_drop = []\n        self.high_corr_pairs = []\n        \n    def fit(self, X, y): \n        X_corr = X.copy()\n        X_corr['isFraud'] = y\n        corr_matrix = X_corr.corr().abs()\n        \n        for i in range(len(corr_matrix.columns)):\n            for j in range(i+1, len(corr_matrix.columns)):\n                \n                if corr_matrix.iloc[i, j] > self.threshold:\n                    self.high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n                    \n        for feat1, feat2, _ in self.high_corr_pairs:\n            if abs(X[feat1].corr(y)) < abs(X[feat2].corr(y)):\n                self.features_to_drop.append(feat1)\n            else:\n                self.features_to_drop.append(feat2)\n        \n        self.features_to_drop = list(set(self.features_to_drop))\n        return self\n\n    \n    def transform(self, X):\n      return X.drop(columns=self.features_to_drop)\n        \n    def fit_transform(self, X, y):\n        return self.fit(X, y).transform(X)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:13.531023Z","iopub.execute_input":"2025-04-27T17:54:13.531314Z","iopub.status.idle":"2025-04-27T17:54:13.559373Z","shell.execute_reply.started":"2025-04-27T17:54:13.531287Z","shell.execute_reply":"2025-04-27T17:54:13.558386Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"pip uninstall scikit-learn imbalanced-learn -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:13.560570Z","iopub.execute_input":"2025-04-27T17:54:13.561000Z","iopub.status.idle":"2025-04-27T17:54:16.715141Z","shell.execute_reply.started":"2025-04-27T17:54:13.560916Z","shell.execute_reply":"2025-04-27T17:54:16.713907Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: imbalanced-learn 0.13.0\nUninstalling imbalanced-learn-0.13.0:\n  Successfully uninstalled imbalanced-learn-0.13.0\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:54:16.716350Z","iopub.execute_input":"2025-04-27T17:54:16.716716Z","iopub.status.idle":"2025-04-27T17:54:24.399993Z","shell.execute_reply.started":"2025-04-27T17:54:16.716682Z","shell.execute_reply":"2025-04-27T17:54:24.398463Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.2.2\n  Downloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting imbalanced-learn==0.10.1\n  Downloading imbalanced_learn-0.10.1-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nDownloading scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m56.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m226.0/226.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: scikit-learn, imbalanced-learn\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.10.1 scikit-learn-1.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import cross_val_score\n\n# Create the pipeline\npipeline = Pipeline([\n    ('missing', DataCleaner()),\n    ('encoding', CustomEncoder()),\n    ('correlation_drop', CorrelationFeatureDropper(threshold=0.8)),\n    ('scaler', StandardScaler()),\n    ('adaboost', AdaBoostClassifier(\n        estimator=DecisionTreeClassifier(max_depth=1),\n        n_estimators=50,\n        learning_rate=1.0,\n        random_state=42\n    ))\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:55:01.511258Z","iopub.execute_input":"2025-04-27T17:55:01.511662Z","iopub.status.idle":"2025-04-27T17:55:01.519805Z","shell.execute_reply.started":"2025-04-27T17:55:01.511635Z","shell.execute_reply":"2025-04-27T17:55:01.518126Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"import mlflow\nfrom sklearn.metrics import (roc_auc_score, f1_score, recall_score, \n                             precision_score, accuracy_score, \n                             average_precision_score, confusion_matrix, \n                             classification_report)\n\n# Set up MLflow experiment\nmlflow.set_experiment(\"AdaBoost_transactions+identity\")\n\n# Start a new run\nwith mlflow.start_run(run_name=\"Basic version using AdaBoost\"):\n    # Log parameters\n    mlflow.log_params({\n        \"model_type\": \"AdaBoost\",\n        \"missing_values\": \"Mean/most freuqent\",\n        \"correlation_drop\": \"CorrelationFeatureDropper(threshold=0.8)\",\n        \"encoding\": \"ordinal encoding + one_hot_encoding(columns with unique<3)\",\n        \"scaler\": \"StandardScaler\",\n        \"n_estimators\": \"50\",\n        \"learning_rate\": \"1.0\"\n        \n    })\n\n    # Fit the pipeline\n    pipeline.fit(X_train, y_train)\n    \n    # Get predictions for all datasets\n    datasets = {\n        'train': (X_train, y_train),\n        'val': (X_val, y_val),\n        #'test': (X_test, y_test)\n    }\n    \n    metrics = {}\n    \n    for dataset_name, (X, y) in datasets.items():\n        # Get probabilities and predictions\n        y_pred_proba = pipeline.predict_proba(X)[:, 1]\n        y_pred = pipeline.predict(X)\n        \n        # Calculate metrics\n        metrics[f\"{dataset_name}_roc_auc\"] = roc_auc_score(y, y_pred_proba)\n        metrics[f\"{dataset_name}_f1\"] = f1_score(y, y_pred)\n        metrics[f\"{dataset_name}_recall\"] = recall_score(y, y_pred)\n        metrics[f\"{dataset_name}_precision\"] = precision_score(y, y_pred)\n        metrics[f\"{dataset_name}_accuracy\"] = accuracy_score(y, y_pred)\n        metrics[f\"{dataset_name}_average_precision\"] = average_precision_score(y, y_pred_proba)\n        \n        # For binary classification, also log metrics for class 1\n        metrics[f\"{dataset_name}_f1_class1\"] = f1_score(y, y_pred, pos_label=1)\n        metrics[f\"{dataset_name}_recall_class1\"] = recall_score(y, y_pred, pos_label=1)\n        metrics[f\"{dataset_name}_precision_class1\"] = precision_score(y, y_pred, pos_label=1)\n        \n        # Print some key metrics\n        print(f\"\\n{dataset_name.upper()} Metrics:\")\n        print(f\"ROC AUC: {metrics[f'{dataset_name}_roc_auc']:.4f}\")\n    \n    \n    # Log all metrics to MLflow\n    mlflow.log_metrics(metrics)\n    \n    # Log the model\n    mlflow.sklearn.log_model(pipeline, \"model\")\n    \n    # Add a tag to identify this as baseline\n    mlflow.set_tag(\"stage\", \"baseline\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T17:55:16.125996Z","iopub.execute_input":"2025-04-27T17:55:16.126351Z","iopub.status.idle":"2025-04-27T18:01:24.542699Z","shell.execute_reply.started":"2025-04-27T17:55:16.126327Z","shell.execute_reply":"2025-04-27T18:01:24.541677Z"}},"outputs":[{"name":"stdout","text":"\nTRAIN Metrics:\nROC AUC: 0.8608\n\nVAL Metrics:\nROC AUC: 0.8586\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/27 18:01:21 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"🏃 View run Basic version using  at: https://dagshub.com/zeliz22/ML_Fraud-Detection.mlflow/#/experiments/3/runs/a87656459f0f4060930b393dea04901e\n🧪 View experiment at: https://dagshub.com/zeliz22/ML_Fraud-Detection.mlflow/#/experiments/3\n","output_type":"stream"}],"execution_count":17}]}