# model_experiment_Basic_Log_regression
დასაწყისისთვის, გავტესტე მარტივი ლოჯისტიკური რეგრესია მხოლოდ train_transactionის საშუალებით, სანამ საქმეში aidentityს ჩავრთავდი. 
* პირველი რანი "initial run" *: მისინგები შევავსე მოდით/მედიანით(დაიდროპა სვეტები, სადაც მისინგები>90%), გამოვიყენე WOE ენქოუდინგი(one_hot იმ სვეტებისთვის, სადაც იუნიქი<3). ასევე გავაკეთე კორელაციის დადროპვა. ვალიდაციაზე roc-auc მივიღე 0.74. 
* მეორე რანი "saving_missing_values" *: შევცვალე მისინგების შევსება: ვივარაუდე, რომ შესაძლოა, მისინგნესი კორელაციაში იყოს დაფრედიქთებასთან, ამიტომ მისინგების პირდაპირ მოდით/მედიანით ჩანაცვლების მაგივრად, ჩავწერე მნიშვნელობები: -999/"MISSING"(დაიდროპა ქოლუმები, სადაც მისინგი >80%). ამან შედეგი უმიშვნელოდ გააუმჯობესა, მხოლოდ 0.1-ით და ავიდე 0.75ზე. 
* მესამე რანი "saving all columns and its missing values" *: რადგან ვნახე, რომ მისინგ ველიუების "შენახვამ" დასწავლისას უკეთესი შედეგი გამოიღო, ვივარაუდე, რომ ის ქოლუმები, რომლებსაც ბევრი მისინგ ველიუ ჰქონდათ და ამის გამო ვშლიდი, შესაძლოა, პირიქით, დაგვხმარებოდა დასწავლის პროცესში, ამიტომ თრეშჰოლდი ავწიე 1ზე. ასევე, ფაიფლაინიდან ამოვიღე კორელაციის ფილტრი, რომ ყველა ქოლუმი შემენარჩუნებინა. ამან მომცა უკეთესი შედეგი: 0.79.
* მეოთხე რანი "saving all columns and its missing values + scaler" *: აქამდე არ ვიყენებდი სქეილერს და დავამატე უბრალოდ სტანდარტ-სქეილერი და ვალიდაციის სქორი ავიდა 0.84მდე, რაც არის ამ ექსპერიმენტის საუკეთესო შედეგი. ამიტომ უკვე გავიშვი X_testზეც სამივე სეტის ქულა დავლოგე მეტრიკად(roc_auc_train = 0.8457 , roc_auc_val = 0.8446, roc_auc_test = 8431)

# model_experiment_Random_Forest_using_transactions
ამ მოდელში გავტესტავ Random Forestის მეთოდს, მაგრამ ჯერ კიდევ დაუმერჯავ დატაზე, მხოლოდ train_transactionების გამოყენებით. 
* პირველი რანი "random forest with standart cleaning method" *: პირველ ვარიანტად დავწერე ყველაზე მარტივი რანდომ ფორესთი, რომელიც მისინგებს ავსებს მოდით/მედიანით და დროპავს ქოლუმებს, რომელშიც მისინგები > 80%. ენქოდერიც ასევე სტანდარტული, უნიკალური<3 ქოლუმებისთის one-hot, და დანარჩენებისთვის WOE. ვალიდაციაზე ავიღე სქორი: 0.9126, მაგრამ თრენინგზე იყო 1.0, ამიტომ სავარაუდოდ მოხდა overfitting
* მეორე რანი "less complex model with undersampling"  *: აქ შევცვალე მსინგების შევსების კლასი და გამოვიყენებ binary flagები, რომ თუკი მისინგნესი კორელაციაში იყო დასაფრედიქთებელ ქოლუმთან, მაინც შენარჩუნებულიყო ეს როგორც ფიჩერი. ასევე, ოვერფიტინგისგან თავის ასარიდებლად, შევამცირე მოდელის კომპლექსურობა(ნაკლები ხეები, ნაკლები სიღრმე, გავუზარდე min_samples_leaf). დავამატე ანდერსემფლინგი და კორელაციის ფილტრიც. roc_auc_train = 0.9204 , roc_auc_val = 0.9004. ტრაინზე დაიწია ქულამ, თუმცა ვალიდაცია მიახლოებულია წინა შედეგთან. მოდელი დიდად არ გაუარესდა და რაც მთავარია, ოვერფიტინგი შემცირდა.

* მესამე რანი "Random forest with RandomizedSearchCV and scaler" *: დავამატე სქეილერი და RandomizedSearch, პარამეტრების გადასარჩევად. ტრეინინგზე მივიღე 0.99. სავარაუდოდ, უმეტესად აირჩია ისეთი პარამეტრები, რამაც ძალიან კომპლექსური გახადა ხე, გააკეთა ბევრი 
* რადგან იყო ამ მოდელში საუკეთესო ვერსია, ასევე გავტესტე X_testზეც. roc_aucები ტრეინზე, ვალიდაციაზე და ტესტებზე, შესაბამისად, მივიღე: 0.92, 0.9005, 0.899
 (ასევე, აქამდე თურმე მარტო ვპრინტავდი F1, Recall და მსგავს მეტრიკებს და დალოგვა მავიწყდებოდა, აქ გავასწორე ეგ და ამის შემდეგ რანებში ეგ მეტრიკებიც დალოგილი იქნება)

# model_experiment_XGBoost_transactions+identity
ამ მოდელში გავტესტავ XGBoost მეთოდს. აქ უკვე, შედეგის გასაუმჯობესებლად, დავმერჯე transactionები და identityის დატასეტები. გამოვიყენე left მერჯი, და რადგან 590K დან მხოლოდ 120K ტრანზაქციაში იყო identity ინფორმაცია, გაჩნდება ბევრი ახალი missing valueები, ამიტომ დატრენინგებისას, სავარაუდოდ დიდი მნიშვნელობა ექნება როგორ შევავსებ ამ მისინგებს და ვეცდები ხშირად შევცვალო და რამდენიმე სხვადასხვა ვარიანტი განვიხილო.
* პირველი რანი "Basic" *: ნალებს ვცვლი ჩვეულებრივ მოდა/საშუალო. ვიყენებ იგივე ენქოდერ კლასს. ჰიპერპარამეტრებში უმეტესად ვუსეტავ დეფოლტ მნიშვნელობებს. მივიღე 0.93 ტრეინზე და 0.92 ვალიდაციაზე. 
* მეორე რანი "filling missings with special values + scaler + more n_estimator": მისინგების შესავსებად ვიყენებ სფეშელ ველიუებს: -999/"Missing". ჰიპერპარამეტრებში გავაკეთე რამდენიმე ცვლილება: n_estimators გავზარდე 500მდე უკეთესი პერფორმანსისთვის. subsample მცირედით გავზარდე ვარიენსის თავიდან ასაცილებლად და ასევე, რაგდან გვაქვს ძალიან არა ბალანსირებული დატა, დავამატე პარამეტრი scale_pos_weight, რომელიც მეტ ყურადღებას მიაქცევს პოზიტივ კლასს და ცოტა შემცირდება ეს იმბალანსი. ამ რანში ტრეინის სქორი ავიდა 0.98ზე და ვალიდაციისა ავიდა 0.95ზე. მართალია ვალიდაციის ქულაც ახლოსაა, მაგრამ მაინც საშიშია ოვერფიტინგი.
* მესამე რანი "": როგორც ზემოთ ავხსენი, მაინც საშიშროება იყო ოვერფიტინგის, ამიტომ  max_depth შევამცირე 4მდე, უკეთესი ჯენერალიზაციისვტის და დავამატე reg_alpha,(L1 რეგულარიზაცია). მივამატე კორელაცია და ანდერსემფლინგი. მისინგებისსთვის ვიყენებ binary flagებს. ((((აქ max_depthის შეცვლა ადაბლებს ქულას, ბოლოს რამდენიმე პარამეტრი შეუცვალე რაც ჩატმა გითხრა მარა ვერ დაელოდე გაშვებას. ფორესტში --- აღადგინე ბოლო ვერსია და იქიდან შეცვლა რამე უაზრობაა უკვე. 





