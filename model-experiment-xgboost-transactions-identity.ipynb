{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install dagshub mlflow","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='zeliz22', repo_name='ML_Fraud-Detection', mlflow=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_transaction =  pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')\ntrain_identity =  pd.read_csv('/kaggle/input/ieee-fraud-detection/train_identity.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_merged = pd.merge(train_transaction, train_identity, on=\"TransactionID\", how=\"left\")\n\nprint(train_merged.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef prepare_data(df, target='isFraud', test_size=0.15, random_state=42):\n    # Split train/val/test\n    train_val, test = train_test_split(\n        df, test_size=test_size, stratify=df[target], random_state=random_state\n    )\n    train, val = train_test_split(\n        train_val, \n        test_size=test_size/(1-test_size),  # Adjust for nested split\n        stratify=train_val[target],\n        random_state=random_state\n    )\n    \n    # Separate X/y\n    def _split(df):\n        return df.drop(columns=[target, 'TransactionID']), df[target]\n    \n    X_train, y_train = _split(train)\n    X_val, y_val = _split(val)\n    X_test, y_test = _split(test)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test\n\nX_train, X_val, X_test, y_train, y_val, y_test = prepare_data(train_merged)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train.shape","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Optional, Union\n\nclass DataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, \n                 numeric_strategy: str = 'mean',\n                 categorical_strategy: str = 'most_frequent',\n                 numeric_fill_value: Optional[Union[int, float]] = None,\n                 categorical_fill_value: Optional[str] = None,\n                 drop_threshold: float = 0.8):\n\n        self.numeric_strategy = numeric_strategy\n        self.categorical_strategy = categorical_strategy\n        self.numeric_fill_value = numeric_fill_value\n        self.categorical_fill_value = categorical_fill_value\n        self.drop_threshold = drop_threshold\n        self.numeric_impute_values_ = {}\n        self.categorical_impute_values_ = {}\n        self.columns_to_drop_ = []\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"Learn imputation values from the data\"\"\"\n        \n        # Identify columns to drop\n        null_ratios = X.isnull().mean()\n        self.columns_to_drop_ = list(null_ratios[null_ratios > self.drop_threshold].index)\n        X_clean = X.drop(columns=self.columns_to_drop_)\n        \n        # Separate numeric and categorical columns\n        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n        \n        # Calculate numeric imputation values\n        for col in numeric_cols:\n            if self.numeric_strategy == 'mean':\n                self.numeric_impute_values_[col] = X_clean[col].mean()\n            elif self.numeric_strategy == 'median':\n                self.numeric_impute_values_[col] = X_clean[col].median()\n            elif self.numeric_strategy == 'constant':\n                if self.numeric_fill_value is None:\n                    raise ValueError(\"numeric_fill_value must be specified for constant strategy\")\n                self.numeric_impute_values_[col] = self.numeric_fill_value\n            elif self.numeric_strategy != 'drop':\n                raise ValueError(f\"Unknown numeric strategy: {self.numeric_strategy}\")\n        \n        # Calculate categorical imputation values\n        for col in categorical_cols:\n            if self.categorical_strategy == 'most_frequent':\n                self.categorical_impute_values_[col] = X_clean[col].mode()[0]\n            elif self.categorical_strategy == 'constant':\n                if self.categorical_fill_value is None:\n                    raise ValueError(\"categorical_fill_value must be specified for constant strategy\")\n                self.categorical_impute_values_[col] = self.categorical_fill_value\n            elif self.categorical_strategy != 'drop':\n                raise ValueError(f\"Unknown categorical strategy: {self.categorical_strategy}\")\n        \n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply the learned imputation to new data\"\"\"\n        \n        # Drop high-null columns\n        X_clean = X.drop(columns=self.columns_to_drop_)\n        \n        # Separate numeric and categorical columns\n        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n        \n        # Apply numeric imputation\n        for col in numeric_cols:\n            if col in self.numeric_impute_values_:\n                X_clean[col] = X_clean[col].fillna(self.numeric_impute_values_[col])\n            elif self.numeric_strategy == 'drop':\n                X_clean = X_clean.dropna(subset=[col])\n        \n        # Apply categorical imputation\n        for col in categorical_cols:\n            if col in self.categorical_impute_values_:\n                X_clean[col] = X_clean[col].fillna(self.categorical_impute_values_[col])\n            elif self.categorical_strategy == 'drop':\n                X_clean = X_clean.dropna(subset=[col])\n        \n        return X_clean\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n        \"\"\"Fit and transform in one step\"\"\"\n        return self.fit(X, y).transform(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mlflow\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass AdvancedDataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, missing_threshold=1):\n        self.missing_threshold = missing_threshold\n        self.numeric_fill = -999\n        self.categorical_fill = \"MISSING\"\n        self.columns_dropped = []\n        self.missing_stats = {}\n        \n    def fit(self, X, y=None):\n        # Calculate missing percentages\n        missing_percent = X.isnull().mean()\n        self.missing_stats = missing_percent.to_dict()\n        \n        # Identify columns to drop\n        self.columns_dropped = list(missing_percent[missing_percent > self.missing_threshold].index)\n        self.columns_kept = [col for col in X.columns if col not in self.columns_dropped]\n        \n        return self\n    \n    def transform(self, X):\n        X = X.copy()\n        \n        # 1. Drop high-missing columns\n        X = X.drop(columns=self.columns_dropped)\n        \n        # 2. Fill remaining missing values\n        num_cols = X.select_dtypes(include=['number']).columns\n        cat_cols = X.select_dtypes(exclude=['number']).columns\n        \n        X[num_cols] = X[num_cols].fillna(self.numeric_fill)\n        X[cat_cols] = X[cat_cols].fillna(self.categorical_fill)\n        \n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass MissingValueHandler(BaseEstimator, TransformerMixin):\n    def __init__(self, num_strategy='median', cat_strategy='most_frequent', \n                 create_flags=True, flag_threshold=0.01, flag_only=False):\n        \"\"\"\n        Optimized missing value handler that avoids fragmentation warnings\n        \n        Parameters same as before\n        \"\"\"\n        self.num_strategy = num_strategy\n        self.cat_strategy = cat_strategy\n        self.create_flags = create_flags\n        self.flag_threshold = flag_threshold\n        self.flag_only = flag_only\n        \n    def fit(self, X, y=None):\n        # Safely detect column types\n        self.num_cols_ = X.select_dtypes(include=np.number).columns.tolist()\n        self.cat_cols_ = X.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        # Initialize storage\n        self.impute_values_ = {}\n        self.flag_cols_ = []\n        \n        # Process all columns with missing values\n        for col in X.columns:\n            missing_ratio = X[col].isna().mean()\n            \n            if missing_ratio > 1e-6:  # Small epsilon to avoid float precision issues\n                # Flag creation logic\n                if self.create_flags and missing_ratio >= self.flag_threshold:\n                    self.flag_cols_.append(col)\n                \n                # Imputation value calculation (unless flag_only)\n                if not self.flag_only:\n                    if col in self.num_cols_:\n                        if self.num_strategy == 'median':\n                            self.impute_values_[col] = X[col].median()\n                        elif self.num_strategy == 'mean':\n                            self.impute_values_[col] = X[col].mean()\n                        else:  # constant\n                            self.impute_values_[col] = 0\n                    elif col in self.cat_cols_:\n                        if self.cat_strategy == 'most_frequent':\n                            # Handle case where mode might be empty\n                            mode = X[col].mode()\n                            self.impute_values_[col] = mode[0] if not mode.empty else 'missing'\n                        else:\n                            self.impute_values_[col] = 'missing'\n        \n        return self\n    \n    def transform(self, X):\n        # Create a single copy upfront\n        X = X.copy()\n        \n        if self.create_flags and len(self.flag_cols_) > 0:\n            # Create all flags at once using pd.concat (more efficient)\n            flag_data = {f'{col}_missing_flag': X[col].isna().astype(np.int8) \n                         for col in self.flag_cols_}\n            X = pd.concat([X, pd.DataFrame(flag_data)], axis=1)\n        \n        # Perform imputation (unless flag_only)\n        if not self.flag_only:\n            for col, value in self.impute_values_.items():\n                if col in X.columns:  # Safety check\n                    X[col] = X[col].fillna(value)\n        \n        return X","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nclass CustomEncoder:\n    def __init__(self, threshold = 3):\n        self.threshold = threshold\n        \n        # Initialize encoders\n        self.one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n        self.ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n        \n        # Store feature names for one-hot encoding\n        self.one_hot_feature_names = None\n        \n    def fit(self, X, y=None):\n\n        cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n        s = X[cat_cols].nunique()\n\n        self.ordinal_cols = list(s[s > self.threshold].index)\n        self.one_hot_cols = list(s[s <= self.threshold].index)\n\n        if self.one_hot_cols:\n            self.one_hot_encoder.fit(X[self.one_hot_cols])\n            self.one_hot_feature_names = self.one_hot_encoder.get_feature_names_out(self.one_hot_cols)\n        \n        if self.ordinal_cols:\n            self.ordinal_encoder.fit(X[self.ordinal_cols])\n        \n        return self\n    \n    def transform(self, X):\n\n        X_transformed = X.copy()\n        \n        # Apply One-Hot Encoding\n        if self.one_hot_cols:\n            one_hot_encoded = self.one_hot_encoder.transform(X[self.one_hot_cols])\n            one_hot_df = pd.DataFrame(one_hot_encoded, columns=self.one_hot_feature_names, index=X.index)\n            X_transformed = pd.concat([X_transformed, one_hot_df], axis=1)\n            X_transformed.drop(self.one_hot_cols, axis=1, inplace=True)\n        \n        # Apply Ordinal Encoding\n        if self.ordinal_cols:\n            ordinal_encoded = self.ordinal_encoder.transform(X[self.ordinal_cols])\n            ordinal_df = pd.DataFrame(ordinal_encoded, columns=self.ordinal_cols, index=X.index)\n            X_transformed[self.ordinal_cols] = ordinal_df\n        \n        return X_transformed\n    \n    def fit_transform(self, X, y = None):\n        return self.fit(X).transform(X)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CorrelationFeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.8):\n        self.threshold = threshold\n        self.features_to_drop = []\n        self.high_corr_pairs = []\n        \n    def fit(self, X, y): \n        X_corr = X.copy()\n        X_corr['isFraud'] = y\n        corr_matrix = X_corr.corr().abs()\n        \n        for i in range(len(corr_matrix.columns)):\n            for j in range(i+1, len(corr_matrix.columns)):\n                \n                if corr_matrix.iloc[i, j] > self.threshold:\n                    self.high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n                    \n        for feat1, feat2, _ in self.high_corr_pairs:\n            if abs(X[feat1].corr(y)) < abs(X[feat2].corr(y)):\n                self.features_to_drop.append(feat1)\n            else:\n                self.features_to_drop.append(feat2)\n        \n        self.features_to_drop = list(set(self.features_to_drop))\n        return self\n\n    \n    def transform(self, X):\n      return X.drop(columns=self.features_to_drop)\n        \n    def fit_transform(self, X, y):\n        return self.fit(X, y).transform(X)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip uninstall scikit-learn imbalanced-learn -y","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nscale_pos = len(y_train[y_train == 0]) / len(y_train[y_train == 1])\n\n# Build the pipeline\npipeline = ImbPipeline(steps=[\n    ('missing', MissingValueHandler()),\n    ('encoding', CustomEncoder()),\n    ('correlation_drop', CorrelationFeatureDropper(threshold=0.8)),\n    ('scaler', StandardScaler()),\n    ('under', RandomUnderSampler(random_state=42)), \n    ('model', XGBClassifier(\n        random_state=42,\n        n_jobs=-1,\n        verbosity=1,  # equivalent to verbose in RF\n        learning_rate=0.1,  # default is 0.3, common to try lower values\n        max_depth=6,  # default is 6, controls tree complexity\n        n_estimators=300,  # number of trees\n        subsample=0.7,  # fraction of samples used per tree\n        colsample_bytree=0.8,  # fraction of features used per tree\n        scale_pos_weight=scale_pos,\n        gamma=0.5,  # minimum loss reduction to make a split\n        min_child_weight=1,  # minimum sum of instance weight needed in a child\n        reg_alpha=2,  # Added L1 regularization\n        \n\n    ))])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import mlflow\nfrom sklearn.metrics import (roc_auc_score, f1_score, recall_score, \n                             precision_score, accuracy_score, \n                             average_precision_score, confusion_matrix, \n                             classification_report)\n\n# Set up MLflow experiment\nmlflow.set_experiment(\"XGBoost_transactions+identity\")\n\n# Start a new run\nwith mlflow.start_run(run_name=\"reduce overfitting, add underSamling and corelation dropper\"):\n    # Log parameters\n    mlflow.log_params({\n        \"model_type\": \"XGBClassifier\",\n        \"missing_values\": \"Mean/most freuqent + binary flags\",\n        \"correlation_drop\": \"CorrelationFeatureDropper(threshold=0.8)\",\n        \"encoding\": \"ordinal encoding + one_hot_encoding(columns with unique<3)\",\n        \"scaler\": \"StandartScaler\",\n        \"sampling\": \"RandomUnderSampler\",\n        \"n_estimators\": \"300\",\n        \"learning_rate\": \"0.05\"\n        \n    })\n\n    # Fit the pipeline\n    pipeline.fit(X_train, y_train)\n    \n    # Get predictions for all datasets\n    datasets = {\n        'train': (X_train, y_train),\n        'val': (X_val, y_val),\n        'test': (X_test, y_test)\n    }\n    \n    metrics = {}\n    \n    for dataset_name, (X, y) in datasets.items():\n        # Get probabilities and predictions\n        y_pred_proba = pipeline.predict_proba(X)[:, 1]\n        y_pred = pipeline.predict(X)\n        \n        # Calculate metrics\n        metrics[f\"{dataset_name}_roc_auc\"] = roc_auc_score(y, y_pred_proba)\n        metrics[f\"{dataset_name}_f1\"] = f1_score(y, y_pred)\n        metrics[f\"{dataset_name}_recall\"] = recall_score(y, y_pred)\n        metrics[f\"{dataset_name}_precision\"] = precision_score(y, y_pred)\n        metrics[f\"{dataset_name}_accuracy\"] = accuracy_score(y, y_pred)\n        metrics[f\"{dataset_name}_average_precision\"] = average_precision_score(y, y_pred_proba)\n        \n        # For binary classification, also log metrics for class 1\n        metrics[f\"{dataset_name}_f1_class1\"] = f1_score(y, y_pred, pos_label=1)\n        metrics[f\"{dataset_name}_recall_class1\"] = recall_score(y, y_pred, pos_label=1)\n        metrics[f\"{dataset_name}_precision_class1\"] = precision_score(y, y_pred, pos_label=1)\n        \n        # Print some key metrics\n        print(f\"\\n{dataset_name.upper()} Metrics:\")\n        print(f\"ROC AUC: {metrics[f'{dataset_name}_roc_auc']:.4f}\")\n    \n    \n    # Log all metrics to MLflow\n    mlflow.log_metrics(metrics)\n    \n    # Log the model\n    mlflow.sklearn.log_model(pipeline, \"model\")\n    \n    # Add a tag to identify this as baseline\n    mlflow.set_tag(\"stage\", \"baseline\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}