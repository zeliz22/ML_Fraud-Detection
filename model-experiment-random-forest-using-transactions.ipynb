{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":14242,"databundleVersionId":568274,"sourceType":"competition"}],"dockerImageVersionId":31012,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:07:52.259899Z","iopub.execute_input":"2025-04-27T18:07:52.260398Z","iopub.status.idle":"2025-04-27T18:07:52.271655Z","shell.execute_reply.started":"2025-04-27T18:07:52.260354Z","shell.execute_reply":"2025-04-27T18:07:52.270299Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ieee-fraud-detection/sample_submission.csv\n/kaggle/input/ieee-fraud-detection/test_identity.csv\n/kaggle/input/ieee-fraud-detection/train_identity.csv\n/kaggle/input/ieee-fraud-detection/test_transaction.csv\n/kaggle/input/ieee-fraud-detection/train_transaction.csv\n","output_type":"stream"}],"execution_count":24},{"cell_type":"code","source":"!pip install dagshub mlflow","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:07:52.273497Z","iopub.execute_input":"2025-04-27T18:07:52.273869Z","iopub.status.idle":"2025-04-27T18:07:57.392655Z","shell.execute_reply.started":"2025-04-27T18:07:52.273841Z","shell.execute_reply":"2025-04-27T18:07:57.391630Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: dagshub in /usr/local/lib/python3.11/dist-packages (0.5.9)\nRequirement already satisfied: mlflow in /usr/local/lib/python3.11/dist-packages (2.22.0)\nRequirement already satisfied: PyYAML>=5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (6.0.2)\nRequirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.4.4)\nRequirement already satisfied: click>=8.0.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (8.1.8)\nRequirement already satisfied: httpx>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.28.1)\nRequirement already satisfied: GitPython>=3.1.29 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.1.44)\nRequirement already satisfied: rich>=13.1.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (14.0.0)\nRequirement already satisfied: dacite~=1.6.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.6.0)\nRequirement already satisfied: tenacity>=8.2.2 in /usr/local/lib/python3.11/dist-packages (from dagshub) (9.0.0)\nRequirement already satisfied: gql[requests] in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.5.2)\nRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.6.7)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.2.3)\nRequirement already satisfied: treelib>=1.6.4 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.7.1)\nRequirement already satisfied: pathvalidate>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.2.3)\nRequirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from dagshub) (2.9.0.post0)\nRequirement already satisfied: boto3 in /usr/local/lib/python3.11/dist-packages (from dagshub) (1.37.29)\nRequirement already satisfied: semver in /usr/local/lib/python3.11/dist-packages (from dagshub) (3.0.4)\nRequirement already satisfied: dagshub-annotation-converter>=0.1.5 in /usr/local/lib/python3.11/dist-packages (from dagshub) (0.1.9)\nRequirement already satisfied: mlflow-skinny==2.22.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.22.0)\nRequirement already satisfied: Flask<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.0)\nRequirement already satisfied: Jinja2<4,>=2.11 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.1.6)\nRequirement already satisfied: alembic!=1.10.0,<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: docker<8,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (7.1.0)\nRequirement already satisfied: graphene<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.4.3)\nRequirement already satisfied: gunicorn<24 in /usr/local/lib/python3.11/dist-packages (from mlflow) (23.0.0)\nRequirement already satisfied: markdown<4,>=3.3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7)\nRequirement already satisfied: matplotlib<4 in /usr/local/lib/python3.11/dist-packages (from mlflow) (3.7.5)\nRequirement already satisfied: numpy<3 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.26.4)\nRequirement already satisfied: pyarrow<20,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (19.0.1)\nRequirement already satisfied: scikit-learn<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.2.2)\nRequirement already satisfied: scipy<2 in /usr/local/lib/python3.11/dist-packages (from mlflow) (1.15.2)\nRequirement already satisfied: sqlalchemy<3,>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow) (2.0.38)\nRequirement already satisfied: cachetools<6,>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (5.5.2)\nRequirement already satisfied: cloudpickle<4 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.1.1)\nRequirement already satisfied: databricks-sdk<1,>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.50.0)\nRequirement already satisfied: fastapi<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.115.12)\nRequirement already satisfied: importlib_metadata!=4.7.0,<9,>=3.7.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (8.6.1)\nRequirement already satisfied: opentelemetry-api<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.16.0)\nRequirement already satisfied: opentelemetry-sdk<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (1.16.0)\nRequirement already satisfied: packaging<25 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (24.2)\nRequirement already satisfied: protobuf<7,>=3.12.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (3.20.3)\nRequirement already satisfied: pydantic<3,>=1.10.8 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.11.3)\nRequirement already satisfied: requests<3,>=2.17.3 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (2.32.3)\nRequirement already satisfied: sqlparse<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.5.3)\nRequirement already satisfied: typing-extensions<5,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (4.13.1)\nRequirement already satisfied: uvicorn<1 in /usr/local/lib/python3.11/dist-packages (from mlflow-skinny==2.22.0->mlflow) (0.34.2)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic!=1.10.0,<2->mlflow) (1.3.9)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (5.3.1)\nRequirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from dagshub-annotation-converter>=0.1.5->dagshub) (11.1.0)\nRequirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from docker<8,>=4.0.0->mlflow) (2.3.0)\nRequirement already satisfied: Werkzeug>=3.1 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (3.1.3)\nRequirement already satisfied: itsdangerous>=2.2 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (2.2.0)\nRequirement already satisfied: blinker>=1.9 in /usr/local/lib/python3.11/dist-packages (from Flask<4->mlflow) (1.9.0)\nRequirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from GitPython>=3.1.29->dagshub) (4.0.12)\nRequirement already satisfied: graphql-core<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.4)\nRequirement already satisfied: graphql-relay<3.3,>=3.1 in /usr/local/lib/python3.11/dist-packages (from graphene<4->mlflow) (3.2.0)\nRequirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.7.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (1.0.7)\nRequirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.23.0->dagshub) (3.10)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.23.0->dagshub) (0.14.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from Jinja2<4,>=2.11->mlflow) (3.0.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (4.56.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (1.4.8)\nRequirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib<4->mlflow) (3.2.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3->mlflow) (2.4.1)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->dagshub) (2025.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->dagshub) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.1.0->dagshub) (2.19.1)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn<2->mlflow) (3.6.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy<3,>=1.4.0->mlflow) (3.1.1)\nRequirement already satisfied: botocore<1.38.0,>=1.37.29 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.37.29)\nRequirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (1.0.1)\nRequirement already satisfied: s3transfer<0.12.0,>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from boto3->dagshub) (0.11.4)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json->dagshub) (0.9.0)\nRequirement already satisfied: yarl<2.0,>=1.6 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.19.0)\nRequirement already satisfied: backoff<3.0,>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (2.2.1)\nRequirement already satisfied: requests-toolbelt<2,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from gql[requests]->dagshub) (1.0.0)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.23.0->dagshub) (1.3.1)\nRequirement already satisfied: google-auth~=2.0 in /usr/local/lib/python3.11/dist-packages (from databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (2.27.0)\nRequirement already satisfied: starlette<0.47.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from fastapi<1->mlflow-skinny==2.22.0->mlflow) (0.46.2)\nRequirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->GitPython>=3.1.29->dagshub) (5.0.2)\nRequirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib_metadata!=4.7.0,<9,>=3.7.0->mlflow-skinny==2.22.0->mlflow) (3.21.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.1.0->dagshub) (0.1.2)\nRequirement already satisfied: deprecated>=1.2.6 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.2.18)\nRequirement already satisfied: setuptools>=16.0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (75.1.0)\nRequirement already satisfied: opentelemetry-semantic-conventions==0.37b0 in /usr/local/lib/python3.11/dist-packages (from opentelemetry-sdk<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (0.37b0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (2.33.1)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10.8->mlflow-skinny==2.22.0->mlflow) (0.4.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.17.3->mlflow-skinny==2.22.0->mlflow) (3.4.1)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json->dagshub) (1.0.0)\nRequirement already satisfied: multidict>=4.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (6.2.0)\nRequirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.6->gql[requests]->dagshub) (0.3.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3->mlflow) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3->mlflow) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from deprecated>=1.2.6->opentelemetry-api<3,>=1.9.0->mlflow-skinny==2.22.0->mlflow) (1.17.2)\nRequirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.4.1)\nRequirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.11/dist-packages (from google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (4.9)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3->mlflow) (2024.2.0)\nRequirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.11/dist-packages (from pyasn1-modules>=0.2.1->google-auth~=2.0->databricks-sdk<1,>=0.20.0->mlflow-skinny==2.22.0->mlflow) (0.6.1)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import dagshub\ndagshub.init(repo_owner='zeliz22', repo_name='ML_Fraud-Detection', mlflow=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:07:57.393913Z","iopub.execute_input":"2025-04-27T18:07:57.394200Z","iopub.status.idle":"2025-04-27T18:07:57.564055Z","shell.execute_reply.started":"2025-04-27T18:07:57.394174Z","shell.execute_reply":"2025-04-27T18:07:57.563038Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Initialized MLflow to track repo \u001b[32m\"zeliz22/ML_Fraud-Detection\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Initialized MLflow to track repo <span style=\"color: #008000; text-decoration-color: #008000\">\"zeliz22/ML_Fraud-Detection\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Repository zeliz22/ML_Fraud-Detection initialized!\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Repository zeliz22/ML_Fraud-Detection initialized!\n</pre>\n"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"train_transaction =  pd.read_csv('/kaggle/input/ieee-fraud-detection/train_transaction.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:07:57.566855Z","iopub.execute_input":"2025-04-27T18:07:57.567189Z","iopub.status.idle":"2025-04-27T18:08:21.979247Z","shell.execute_reply.started":"2025-04-27T18:07:57.567164Z","shell.execute_reply":"2025-04-27T18:08:21.978253Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\ndef prepare_data(df, target='isFraud', test_size=0.15, random_state=42):\n    # Split train/val/test\n    train_val, test = train_test_split(\n        df, test_size=test_size, stratify=df[target], random_state=random_state\n    )\n    train, val = train_test_split(\n        train_val, \n        test_size=test_size/(1-test_size),  # Adjust for nested split\n        stratify=train_val[target],\n        random_state=random_state\n    )\n    \n    # Separate X/y\n    def _split(df):\n        return df.drop(columns=[target, 'TransactionID']), df[target]\n    \n    X_train, y_train = _split(train)\n    X_val, y_val = _split(val)\n    X_test, y_test = _split(test)\n    \n    return X_train, X_val, X_test, y_train, y_val, y_test\n\nX_train, X_val, X_test, y_train, y_val, y_test = prepare_data(train_transaction)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:21.980376Z","iopub.execute_input":"2025-04-27T18:08:21.980747Z","iopub.status.idle":"2025-04-27T18:08:31.930404Z","shell.execute_reply.started":"2025-04-27T18:08:21.980714Z","shell.execute_reply":"2025-04-27T18:08:31.927482Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"from sklearn.base import BaseEstimator, TransformerMixin\nimport pandas as pd\nimport numpy as np\nfrom typing import Dict, Optional, Union\n\nclass DataCleaner(BaseEstimator, TransformerMixin):\n    def __init__(self, \n                 numeric_strategy: str = 'mean',\n                 categorical_strategy: str = 'most_frequent',\n                 numeric_fill_value: Optional[Union[int, float]] = None,\n                 categorical_fill_value: Optional[str] = None,\n                 drop_threshold: float = 0.8):\n\n        self.numeric_strategy = numeric_strategy\n        self.categorical_strategy = categorical_strategy\n        self.numeric_fill_value = numeric_fill_value\n        self.categorical_fill_value = categorical_fill_value\n        self.drop_threshold = drop_threshold\n        self.numeric_impute_values_ = {}\n        self.categorical_impute_values_ = {}\n        self.columns_to_drop_ = []\n\n    def fit(self, X: pd.DataFrame, y=None):\n        \"\"\"Learn imputation values from the data\"\"\"\n        \n        # Identify columns to drop\n        null_ratios = X.isnull().mean()\n        self.columns_to_drop_ = list(null_ratios[null_ratios > self.drop_threshold].index)\n        X_clean = X.drop(columns=self.columns_to_drop_)\n        \n        # Separate numeric and categorical columns\n        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n        \n        # Calculate numeric imputation values\n        for col in numeric_cols:\n            if self.numeric_strategy == 'mean':\n                self.numeric_impute_values_[col] = X_clean[col].mean()\n            elif self.numeric_strategy == 'median':\n                self.numeric_impute_values_[col] = X_clean[col].median()\n            elif self.numeric_strategy == 'constant':\n                if self.numeric_fill_value is None:\n                    raise ValueError(\"numeric_fill_value must be specified for constant strategy\")\n                self.numeric_impute_values_[col] = self.numeric_fill_value\n            elif self.numeric_strategy != 'drop':\n                raise ValueError(f\"Unknown numeric strategy: {self.numeric_strategy}\")\n        \n        # Calculate categorical imputation values\n        for col in categorical_cols:\n            if self.categorical_strategy == 'most_frequent':\n                self.categorical_impute_values_[col] = X_clean[col].mode()[0]\n            elif self.categorical_strategy == 'constant':\n                if self.categorical_fill_value is None:\n                    raise ValueError(\"categorical_fill_value must be specified for constant strategy\")\n                self.categorical_impute_values_[col] = self.categorical_fill_value\n            elif self.categorical_strategy != 'drop':\n                raise ValueError(f\"Unknown categorical strategy: {self.categorical_strategy}\")\n        \n        return self\n\n    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n        \"\"\"Apply the learned imputation to new data\"\"\"\n        \n        # Drop high-null columns\n        X_clean = X.drop(columns=self.columns_to_drop_)\n        \n        # Separate numeric and categorical columns\n        numeric_cols = X_clean.select_dtypes(include=np.number).columns\n        categorical_cols = X_clean.select_dtypes(exclude=np.number).columns\n        \n        # Apply numeric imputation\n        for col in numeric_cols:\n            if col in self.numeric_impute_values_:\n                X_clean[col] = X_clean[col].fillna(self.numeric_impute_values_[col])\n            elif self.numeric_strategy == 'drop':\n                X_clean = X_clean.dropna(subset=[col])\n        \n        # Apply categorical imputation\n        for col in categorical_cols:\n            if col in self.categorical_impute_values_:\n                X_clean[col] = X_clean[col].fillna(self.categorical_impute_values_[col])\n            elif self.categorical_strategy == 'drop':\n                X_clean = X_clean.dropna(subset=[col])\n        \n        return X_clean\n\n    def fit_transform(self, X: pd.DataFrame, y=None) -> pd.DataFrame:\n        \"\"\"Fit and transform in one step\"\"\"\n        return self.fit(X, y).transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:31.933414Z","iopub.execute_input":"2025-04-27T18:08:31.935411Z","iopub.status.idle":"2025-04-27T18:08:31.967588Z","shell.execute_reply.started":"2025-04-27T18:08:31.935309Z","shell.execute_reply":"2025-04-27T18:08:31.966378Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass MissingValueHandler(BaseEstimator, TransformerMixin):\n    def __init__(self, num_strategy='median', cat_strategy='most_frequent', \n                 create_flags=True, flag_threshold=0.01, flag_only=False):\n        \"\"\"\n        Optimized missing value handler that avoids fragmentation warnings\n        \n        Parameters same as before\n        \"\"\"\n        self.num_strategy = num_strategy\n        self.cat_strategy = cat_strategy\n        self.create_flags = create_flags\n        self.flag_threshold = flag_threshold\n        self.flag_only = flag_only\n        \n    def fit(self, X, y=None):\n        # Safely detect column types\n        self.num_cols_ = X.select_dtypes(include=np.number).columns.tolist()\n        self.cat_cols_ = X.select_dtypes(include=['object', 'category']).columns.tolist()\n        \n        # Initialize storage\n        self.impute_values_ = {}\n        self.flag_cols_ = []\n        \n        # Process all columns with missing values\n        for col in X.columns:\n            missing_ratio = X[col].isna().mean()\n            \n            if missing_ratio > 1e-6:  # Small epsilon to avoid float precision issues\n                # Flag creation logic\n                if self.create_flags and missing_ratio >= self.flag_threshold:\n                    self.flag_cols_.append(col)\n                \n                # Imputation value calculation (unless flag_only)\n                if not self.flag_only:\n                    if col in self.num_cols_:\n                        if self.num_strategy == 'median':\n                            self.impute_values_[col] = X[col].median()\n                        elif self.num_strategy == 'mean':\n                            self.impute_values_[col] = X[col].mean()\n                        else:  # constant\n                            self.impute_values_[col] = 0\n                    elif col in self.cat_cols_:\n                        if self.cat_strategy == 'most_frequent':\n                            # Handle case where mode might be empty\n                            mode = X[col].mode()\n                            self.impute_values_[col] = mode[0] if not mode.empty else 'missing'\n                        else:\n                            self.impute_values_[col] = 'missing'\n        \n        return self\n    \n    def transform(self, X):\n        # Create a single copy upfront\n        X = X.copy()\n        \n        if self.create_flags and len(self.flag_cols_) > 0:\n            # Create all flags at once using pd.concat (more efficient)\n            flag_data = {f'{col}_missing_flag': X[col].isna().astype(np.int8) \n                         for col in self.flag_cols_}\n            X = pd.concat([X, pd.DataFrame(flag_data)], axis=1)\n        \n        # Perform imputation (unless flag_only)\n        if not self.flag_only:\n            for col, value in self.impute_values_.items():\n                if col in X.columns:  # Safety check\n                    X[col] = X[col].fillna(value)\n        \n        return X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:31.968832Z","iopub.execute_input":"2025-04-27T18:08:31.969129Z","iopub.status.idle":"2025-04-27T18:08:31.997039Z","shell.execute_reply.started":"2025-04-27T18:08:31.969107Z","shell.execute_reply":"2025-04-27T18:08:31.996011Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder\n\nclass CustomEncoder:\n    def __init__(self, threshold = 3):\n        self.threshold = threshold\n        \n        # Initialize encoders\n        self.one_hot_encoder = OneHotEncoder(sparse_output=False, handle_unknown='ignore')\n        self.ordinal_encoder = OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)\n        \n        # Store feature names for one-hot encoding\n        self.one_hot_feature_names = None\n        \n    def fit(self, X, y=None):\n\n        cat_cols = [col for col in X.columns if X[col].dtype == 'object']\n        s = X[cat_cols].nunique()\n\n        self.ordinal_cols = list(s[s > self.threshold].index)\n        self.one_hot_cols = list(s[s <= self.threshold].index)\n\n        if self.one_hot_cols:\n            self.one_hot_encoder.fit(X[self.one_hot_cols])\n            self.one_hot_feature_names = self.one_hot_encoder.get_feature_names_out(self.one_hot_cols)\n        \n        if self.ordinal_cols:\n            self.ordinal_encoder.fit(X[self.ordinal_cols])\n        \n        return self\n    \n    def transform(self, X):\n\n        X_transformed = X.copy()\n        \n        # Apply One-Hot Encoding\n        if self.one_hot_cols:\n            one_hot_encoded = self.one_hot_encoder.transform(X[self.one_hot_cols])\n            one_hot_df = pd.DataFrame(one_hot_encoded, columns=self.one_hot_feature_names, index=X.index)\n            X_transformed = pd.concat([X_transformed, one_hot_df], axis=1)\n            X_transformed.drop(self.one_hot_cols, axis=1, inplace=True)\n        \n        # Apply Ordinal Encoding\n        if self.ordinal_cols:\n            ordinal_encoded = self.ordinal_encoder.transform(X[self.ordinal_cols])\n            ordinal_df = pd.DataFrame(ordinal_encoded, columns=self.ordinal_cols, index=X.index)\n            X_transformed[self.ordinal_cols] = ordinal_df\n        \n        return X_transformed\n    \n    def fit_transform(self, X, y = None):\n        return self.fit(X).transform(X)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:31.998201Z","iopub.execute_input":"2025-04-27T18:08:31.998587Z","iopub.status.idle":"2025-04-27T18:08:32.016064Z","shell.execute_reply.started":"2025-04-27T18:08:31.998563Z","shell.execute_reply":"2025-04-27T18:08:32.014887Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\nclass CorrelationFeatureDropper(BaseEstimator, TransformerMixin):\n    def __init__(self, threshold=0.8):\n        self.threshold = threshold\n        self.features_to_drop = []\n        self.high_corr_pairs = []\n        \n    def fit(self, X, y): \n        X_corr = X.copy()\n        X_corr['isFraud'] = y\n        corr_matrix = X_corr.corr().abs()\n        \n        for i in range(len(corr_matrix.columns)):\n            for j in range(i+1, len(corr_matrix.columns)):\n                \n                if corr_matrix.iloc[i, j] > self.threshold:\n                    self.high_corr_pairs.append((corr_matrix.columns[i], corr_matrix.columns[j], corr_matrix.iloc[i, j]))\n                    \n        for feat1, feat2, _ in self.high_corr_pairs:\n            if abs(X[feat1].corr(y)) < abs(X[feat2].corr(y)):\n                self.features_to_drop.append(feat1)\n            else:\n                self.features_to_drop.append(feat2)\n        \n        self.features_to_drop = list(set(self.features_to_drop))\n        return self\n\n    \n    def transform(self, X):\n      return X.drop(columns=self.features_to_drop)\n        \n    def fit_transform(self, X, y):\n        return self.fit(X, y).transform(X)\n    ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:32.017328Z","iopub.execute_input":"2025-04-27T18:08:32.017601Z","iopub.status.idle":"2025-04-27T18:08:32.049039Z","shell.execute_reply.started":"2025-04-27T18:08:32.017580Z","shell.execute_reply":"2025-04-27T18:08:32.047580Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"pip uninstall scikit-learn imbalanced-learn -y","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:32.052009Z","iopub.execute_input":"2025-04-27T18:08:32.052419Z","iopub.status.idle":"2025-04-27T18:08:34.128526Z","shell.execute_reply.started":"2025-04-27T18:08:32.052392Z","shell.execute_reply":"2025-04-27T18:08:34.123806Z"}},"outputs":[{"name":"stdout","text":"Found existing installation: scikit-learn 1.2.2\nUninstalling scikit-learn-1.2.2:\n  Successfully uninstalled scikit-learn-1.2.2\nFound existing installation: imbalanced-learn 0.10.1\nUninstalling imbalanced-learn-0.10.1:\n  Successfully uninstalled imbalanced-learn-0.10.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"pip install scikit-learn==1.2.2 imbalanced-learn==0.10.1","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:34.134354Z","iopub.execute_input":"2025-04-27T18:08:34.135645Z","iopub.status.idle":"2025-04-27T18:08:44.218789Z","shell.execute_reply.started":"2025-04-27T18:08:34.135451Z","shell.execute_reply":"2025-04-27T18:08:44.217321Z"}},"outputs":[{"name":"stdout","text":"Collecting scikit-learn==1.2.2\n  Using cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nCollecting imbalanced-learn==0.10.1\n  Using cached imbalanced_learn-0.10.1-py3-none-any.whl.metadata (8.2 kB)\nRequirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.26.4)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn==1.2.2) (3.6.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17.3->scikit-learn==1.2.2) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17.3->scikit-learn==1.2.2) (2024.2.0)\nUsing cached scikit_learn-1.2.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.6 MB)\nUsing cached imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\nInstalling collected packages: scikit-learn, imbalanced-learn\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nnilearn 0.11.1 requires scikit-learn>=1.4.0, but you have scikit-learn 1.2.2 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nmlxtend 0.23.4 requires scikit-learn>=1.3.1, but you have scikit-learn 1.2.2 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed imbalanced-learn-0.10.1 scikit-learn-1.2.2\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":34},{"cell_type":"code","source":"from sklearn.feature_selection import RFE\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import roc_auc_score, classification_report\nfrom imblearn.pipeline import Pipeline as ImbPipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import RandomizedSearchCV\n\n\n# Base model\nrf = RandomForestClassifier(\n    random_state=42,\n    n_estimators=100,          # Increase from 50 to 100\n    max_depth=15,              # Try deeper trees for complex patterns\n    min_samples_split=5,       # Slightly more flexible splitting\n    min_samples_leaf=2,        # Allow smaller leaf nodes\n    max_features='sqrt',       # Standard choice for classification\n    max_samples=0.8,           # Keep this as is\n    class_weight='balanced',   # Try simple balanced weighting\n    n_jobs=-1,\n    verbose=1\n)\n\n# Build the pipeline\npipeline = ImbPipeline(steps=[\n    ('missing', MissingValueHandler()),\n    ('encoding', CustomEncoder()),\n    ('correlation_drop', CorrelationFeatureDropper(threshold=0.8)),\n    ('scaler', StandardScaler()),\n    ('sampling', SMOTE(random_state=42)),\n    ('model', rf)\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:44.220320Z","iopub.execute_input":"2025-04-27T18:08:44.220664Z","iopub.status.idle":"2025-04-27T18:08:44.231036Z","shell.execute_reply.started":"2025-04-27T18:08:44.220617Z","shell.execute_reply":"2025-04-27T18:08:44.229740Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"import mlflow\nfrom sklearn.metrics import (roc_auc_score, f1_score, recall_score, \n                             precision_score, accuracy_score, \n                             average_precision_score, confusion_matrix, \n                             classification_report)\n\nmlflow.set_experiment(\"Random_Forest_using_transactions\")\n\n# Start a new run\nwith mlflow.start_run(run_name=\"using smoteSamoling, increased n_estimators\"):\n    # Log parameters\n    mlflow.log_params({\n        \"model_type\": \"RandomForestClassifier\",\n        \"missing_values\": \"Mean/most freuqent + binary flags\",\n        \"encoding\": \"ordinal encoding + one_hot_encoding(columns with unique<3)\",\n        \"sampling\": \"SMOTE\",\n        \"n_estimators\": \"100\",\n        \"max_depth\": \"15\"\n        \n    })\n    # Fit the pipeline\n    pipeline.fit(X_train, y_train)\n    \n    # Get predictions for all datasets\n    datasets = {\n        'train': (X_train, y_train),\n        'val': (X_val, y_val),\n        'test': (X_test, y_test)\n    }\n    \n    metrics = {}\n    \n    for dataset_name, (X, y) in datasets.items():\n        # Get probabilities and predictions\n        y_pred_proba = pipeline.predict_proba(X)[:, 1]\n        y_pred = pipeline.predict(X)\n        \n        # Calculate metrics\n        metrics[f\"{dataset_name}_roc_auc\"] = roc_auc_score(y, y_pred_proba)\n        metrics[f\"{dataset_name}_f1\"] = f1_score(y, y_pred)\n        metrics[f\"{dataset_name}_recall\"] = recall_score(y, y_pred)\n        metrics[f\"{dataset_name}_precision\"] = precision_score(y, y_pred)\n        metrics[f\"{dataset_name}_accuracy\"] = accuracy_score(y, y_pred)\n        metrics[f\"{dataset_name}_average_precision\"] = average_precision_score(y, y_pred_proba)\n        \n        # For binary classification, also log metrics for class 1\n        metrics[f\"{dataset_name}_f1_class1\"] = f1_score(y, y_pred, pos_label=1)\n        metrics[f\"{dataset_name}_recall_class1\"] = recall_score(y, y_pred, pos_label=1)\n        metrics[f\"{dataset_name}_precision_class1\"] = precision_score(y, y_pred, pos_label=1)\n        \n        # Print some key metrics\n        print(f\"\\n{dataset_name.upper()} Metrics:\")\n        print(f\"ROC AUC: {metrics[f'{dataset_name}_roc_auc']:.4f}\")\n    \n    \n    # Log all metrics to MLflow\n    mlflow.log_metrics(metrics)\n    \n    # Log the model\n    mlflow.sklearn.log_model(pipeline, \"model\")\n    \n    # Add a tag to identify this as baseline\n    mlflow.set_tag(\"stage\", \"baseline\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-27T18:08:44.232396Z","iopub.execute_input":"2025-04-27T18:08:44.232817Z","iopub.status.idle":"2025-04-27T18:27:03.593465Z","shell.execute_reply.started":"2025-04-27T18:08:44.232782Z","shell.execute_reply":"2025-04-27T18:27:03.592022Z"}},"outputs":[{"name":"stderr","text":"[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.0min\n[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  2.4min finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.7s\n[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    4.0s finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    1.8s\n[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    4.1s finished\n","output_type":"stream"},{"name":"stdout","text":"\nTRAIN Metrics:\nROC AUC: 0.9450\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n","output_type":"stream"},{"name":"stdout","text":"\nVAL Metrics:\nROC AUC: 0.9118\n","output_type":"stream"},{"name":"stderr","text":"[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.3s\n[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n[Parallel(n_jobs=4)]: Using backend ThreadingBackend with 4 concurrent workers.\n[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:    0.4s\n[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:    0.8s finished\n","output_type":"stream"},{"name":"stdout","text":"\nTEST Metrics:\nROC AUC: 0.9115\n","output_type":"stream"},{"name":"stderr","text":"\u001b[31m2025/04/27 18:27:00 WARNING mlflow.models.model: Model logged without a signature and input example. Please set `input_example` parameter when logging the model to auto infer the model signature.\u001b[0m\n","output_type":"stream"},{"name":"stdout","text":"ðŸƒ View run using smoteSamoling, increased n_estimators at: https://dagshub.com/zeliz22/ML_Fraud-Detection.mlflow/#/experiments/1/runs/ba81f0007cc74d1d8fc171051874cf98\nðŸ§ª View experiment at: https://dagshub.com/zeliz22/ML_Fraud-Detection.mlflow/#/experiments/1\n","output_type":"stream"}],"execution_count":36}]}